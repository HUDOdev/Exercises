{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise we discussed the mechanisms of gradient backpropagation and implemented the corresponding functionality in the ``toolbox`` module. This exercise introduces PyTorch, a Deep Learning framework providing powerful tools for simple and efficient implementations of neural networks.\n",
    "\n",
    "First, we need to import the corresponding Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic building block of PyTorch are Tensors, which behave very similar to the class ``toolbox.Tensor`` from last week. Tensors can be initialized by passing a NumPy array or by calling dedicated functions with syntax similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[1., 2.], [3., 4.]]))\n",
    "b = torch.ones(2, 2)\n",
    "c = torch.empty(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1.0102e-38, 8.9082e-39, 8.9082e-39, 1.0194e-38, 9.1837e-39])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch features the same mechanism for gradient backpropagation as developed in last week's exercise and even provides the same attribute names:\n",
    "- ``data`` stores the internal data of the tensor representing its value.\n",
    "- ``requires_grad`` specifies if the tensor participates in the build-up of the computation graph.\n",
    "- ``grad`` keeps a tensor storing the gradient of the last backpropagation for leaf nodes of the graph with ``requires_grad=True``.\n",
    "- ``grad_fn`` points to the operation generating the tensor; ``None`` if the tensor is a leaf node.\n",
    "\n",
    "Again, the backpropagation is initiated by calling ``backward`` on a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = c + a\n",
    "d = d * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(16.)\n",
      "<AddBackward0 object at 0x000001791A20CCA0>\n",
      "<MulBackward0 object at 0x000001791435EE60>\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)\n",
    "print(a.grad)\n",
    "print(c.grad_fn)\n",
    "print(d.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, computation graphs will not be held alive causing errors when calling ``backward`` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d.backward()\n",
    "except:\n",
    "    print(\"Not working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time one usually does not want to bulild up the computation graph for calculating the gradients as it's computationally expensive and bears the danger of memory leakage. Therefore PyTorch provides the context manager ``torch.no_grad`` for disabling gradient computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working...\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = a + b\n",
    "\n",
    "try:\n",
    "    c.backward()\n",
    "except:\n",
    "    print(\"Not working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA [Only works on systems with a GPU - not required for the graded task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, all computations in this exercise class were carried out on a CPU. The hardware of choice for Deep Learning applications, however, are GPUs. The high amount of parallelization and efficient calculation of simple operations on GPUs are highly beneficial for training and inference in artificial neural networks. Therefore, all major Deep Learning frameworks offer the possibility of utilizing GPUs.\n",
    "\n",
    "In PyTorch we can move a tensor to the GPU by simply calling the function ``cuda`` returning a pointer to the tensor. The device of a tensor can be determined using the ``device`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl_22\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "a = a.cuda()\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, ``cuda`` means that the tensor is located on a GPU and the number after the colon specifies the device among all available GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are one driving force in the field of machine learning and therefore it is important to deal with data properly. Luckily, PyTorch provides some handy tools which make the treatment of data very easy. The base class for representing data sets is ``torch.utils.data.Dataset``. This class overloads the functions ``__len__`` and ``__getitem__`` which allow us to easily access data contained in the set as we know it for example from Pythons lists.\n",
    "\n",
    "PyTorch already comprises wrapper classes for some poular data sets in ``torchvision.datasets``. To get started, we will use the MNIST data set of handwritten digits from LeCun et al. http://yann.lecun.com/exdb/mnist/. The argument ``transform`` below forces the data to be converted to PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mMNIST\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[0;32m      2\u001b[0m test_set \u001b[38;5;241m=\u001b[39m MNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MNIST' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = MNIST('data', download=True, transform=transforms.ToTensor())\n",
    "test_set = MNIST('data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single elements of these data sets are tuples containing an image representing a handwritten digit stored as an array in a PyTorch Tensor and the corresponding value of the digit as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "el = train_set[0]\n",
    "print(type(el))\n",
    "print(el[0].shape)\n",
    "print(el[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know the value of the digit represented in the image is 5. We can show the corresponding image using ``matplotlib.pyplot.imshow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 5')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(el[0][0], cmap='gray', vmin=0., vmax=1.)\n",
    "plt.title('Label: {}'.format(el[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``imshow`` requires grayscale images to be of size $H\\times W$ but ``el`` is of size $1\\times H \\times W$. The reason therefor is that images are in general represented as $C \\times H \\times W$ arrays, where $C$ denotes the number of channels of the image. For color images $C$ typically amounts to $3$, for grayscale images to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equally important as the data itself is the process of loading it efficiently to the processing unit. PyTorch provides an easy interface for tackling this problem. Most of the time, we also want mini-batches of data rather than single elements of our data set. The class ``DataLoader`` from ``torch.utils.data`` takes care of this and we only have to provide the data set and define the batch size. Additionally, we may specify further arguments like whether we want the data to be shuffled or the number of workers involved in the process of loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Instance and class checks can only be used with @runtime_checkable protocols",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsc_22\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:200\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiprocessing_context \u001b[38;5;241m=\u001b[39m multiprocessing_context\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Arg-check dataset related before checking samplers because we want to\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# tell users that iterable-style datasets are incompatible with custom\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# samplers first, so that they don't learn that this combo doesn't work\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# after spending time fixing the custom sampler errors.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterableDataset\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m=\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# NOTE [ Custom Samplers and IterableDataset ]\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# `IterableDataset` does not support custom `batch_sampler` or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# this, and support custom samplers that specify the assignments to\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# specific workers.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsc_22\\lib\\typing.py:1498\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;66;03m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m     \u001b[38;5;66;03m# assigned in __init__.\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1494\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_runtime_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m _allow_reckless_class_checks(depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1497\u001b[0m     ):\n\u001b[1;32m-> 1498\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstance and class checks can only be used with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1499\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m @runtime_checkable protocols\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m             _is_callable_members_only(\u001b[38;5;28mcls\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mcls\u001b[39m)):\n\u001b[0;32m   1504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Instance and class checks can only be used with @runtime_checkable protocols"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "data = batch[0]\n",
    "labels = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batch))\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the array has increased by the first dimension related to the differnt batch elements, i.e. the general shape for images in mini-batches is $B \\times C \\times H \\times W$ where $B$ is the batch size. Let's visualize the images of this mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m      2\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mdata\u001b[49m[i, \u001b[38;5;241m0\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m)\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(labels[i]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(data[i, 0], cmap='gray', vmin=0., vmax=1.)\n",
    "    plt.title('Label: {}'.format(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements are randomly shuffled each time we iterate over the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our ``toolbox.Module``, PyTorch defines ``Module`` in ``torch.nn`` which serves as base class for neural networks and their layers. For defining a network, you have to inherit from ``torch.nn.Module`` and define the ``forward`` function. Furthermore, ``torch.nn`` provides you with a broad variety of pre-implemented tools and layers you can use in your network. To get a first impression, have a look at the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 20)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this: In the constructor we instantiate a couple of layers we want to use in the forward pass. For the linear layer we have to specifty the input and output dimensions of our linear transformations. The batch sizes usually are excluded from these size considerations as the data is procesed separately for each element in the mini-batch. To shed light on this mechanism, consider ``layer3``. It expects inputs of size $B \\times 20$, multiplies each element of the batch with a matrix of size $10 \\times 20$ (and possibly adds some vector afterwards) and outputs the transformed data of size $B \\times 10$.\n",
    "\n",
    "In order to be able to pass an image to a linear layer, we have to resize the image with the dimensions $B \\times C \\times H \\times W$ to a vector of size $B \\times \\left( C \\cdot H \\cdot W \\right)$. The forward pass of an object of type ``torch.nn.Module`` can be performed by simply calling the object itself like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = network(batch[0])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4])\n",
      "tensor([3, 1, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "_, pred_labels = output.max(dim=1)\n",
    "\n",
    "print(pred_labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the output of the last layer to be of size ten as we want to predict one out of ten labels. We would like our network to assign the highest value of the output on the coordinate corresponding to the correct label. As discussed earlier, the cross entropy loss is suitable for classification tasks penalizing high output values on the wrong labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3013)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fun(output, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as it comes to training, we want to modify the network's parameters in order to decrease the loss on the training set. We can access the learnable parameters of a network by calling its ``parameters`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = network.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the parameters of the network are the weights and biases of the two linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding multiple layers to a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of layers increases, adding every single layer to the attribute list of the network and explicitly iterating over them in the forward pass can be tedious. A convenient alternative in this case is PyTorch's container class ``torch.nn.Sequential`` which allows you to condense multiple layers into a single ``Module`` object. The usage is straight-forward as you can see in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(28*28, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** It is not sufficient to store the network's layers in a list as we did in our own ``Network`` class from the second last exercise class. Even though you would still be able to perform a forward pass, the layers are not registered as submodules of the network object and therefore their parameters can't be accessed using the ``parameters`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU [Only works on systems with a GPU - not required for the graded task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform training or inference of a network on a GPU we have to move the data as well as the network itself to the GPU. This again works by simply calling the ``cuda`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data, labels \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mcuda(), labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      2\u001b[0m network \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Lars-\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "data, labels = data.cuda(), labels.cuda()\n",
    "network = network.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = network(data)\n",
    "    loss = loss_fun(output, labels)\n",
    "\n",
    "print(output.device)\n",
    "print(loss.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network using SGD [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD in ``optimizer.py`` as introduced in the lecture and do not use the SGD optimizer included in PyTorch. Test your implementation by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import SGD\n",
    "\n",
    "net = SimpleNet2()\n",
    "\n",
    "net.load_state_dict(torch.load('net_params'))\n",
    "\n",
    "batch = next(iter(DataLoader(train_set, batch_size=4)))\n",
    "\n",
    "op = net(batch[0])\n",
    "\n",
    "l = loss_fun(op, batch[1])\n",
    "\n",
    "l.backward()\n",
    "\n",
    "optim = SGD(net.parameters())\n",
    "\n",
    "optim.step()\n",
    "\n",
    "state_dict = net.state_dict()\n",
    "target_state_dict = torch.load('net_params_updated')\n",
    "\n",
    "for key in state_dict:\n",
    "    assert (state_dict[key] - target_state_dict[key]).abs().max() < 1e-6 # 1 point\n",
    "\n",
    "optim.zero_grad()\n",
    "\n",
    "for param in net.parameters():\n",
    "    assert param.grad.abs().max() == 0 # 1 point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa8dd869326b64c7db28626ec0c0af5ccfa916b03840e40712ed321847a3d69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
